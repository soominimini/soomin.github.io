<!DOCTYPE HTML>
<!--
	Helios by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>


	<style>
		div#square{
			position: absolute;
			background-color:transparent;
			outline-color: red;
			outline-style: solid;
			width: 250px;
			height: 250px;

			animation-name: fadein;
			animation-duration: 3s;
			animation-iteration-count: infinite;
		}
		@keyframes fadein {
				from {
					opacity:0;
				}
				to {
					opacity:1;
				}
			}
a:link {
  color: green;
  background-color: transparent;
  text-decoration: none;
}
a:hover {
  color: red;
  background-color: transparent;
  text-decoration: underline;
}
a:active {
  color: yellow;
  background-color: transparent;
  text-decoration: underline;
}
	</style>
	<title>Portfolio_emotionC</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" />
	</noscript>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9PP3T9W5Q9"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9PP3T9W5Q9');
</script>

</head>

<body class="no-sidebar is-preload">
	<div id="page-wrapper">

		<!-- Header -->
		<div id="header">

			<!-- Inner -->
			<div class="inner">
				<header>
					<h1><a href="index.html" id="logo">Portfolio of Soomin Shin</a></h1>
				</header>
			</div>

			<!-- Nav -->
			<nav id="nav">
				<ul>
					<li><a href="index.html">Home</a></li>
					<li>
						<a href="#">Project list</a>
						<ul>
							<li><a href="./P2.html">Empathy in humans towards a robot</a></li>
							<li><a href="./P3.html">What information do people use when perceving others' emotions?</a></li>
							<li><a href="./P4.html">The different mechanism between Convolutional Neural Networks and Humans in Emotion Recognition</a></li>
							<li><a href="./P1.html">Sentiment analysis system for psychological examinaiton</a></li>
							
							<li><a href="./P5.html">Teaching children programming via robotics</a></li>
							<li><a href="./P7.html">Is block programming effective for adults when learning general programming languages?</a></li>
							<li><a href="./P9.html">Can machine learning agents learn a social behavior?</a></li>
							<li><a href="./P8.html">An Android application matches crowdfunding products to users based on their personality</a></li>
							<li><a href="./P6.html">works resulting from the internship program </a></li>
						</ul>
					</li>
					<!-- <li><a href="AboutMe.html">About Me</a></li> -->
				</ul>
			</nav>

		</div>

		<!-- Main -->
		<div class="wrapper style1">

			<div class="container" style="font-family: 'Times New Roman', Times, serif;">
				<article id="main" class="special">
					<header>
						<h2>Different mechanism between Convolutional Neural Networks and Humans in Emotional Recognition</h2>
						<p>
						<a href="https://dl.acm.org/doi/abs/10.1145/3536220.3558036"><i> Published paper at 24th ACM International Conference on Multimodal Interaction (ICMI) Workshop</i></a>	
						</p>
					</header>

					<div class="row">
						<h3>Can Deep Neural Network utilize contextual information to infer emotions as human does?</h3>
						<article class="col-8 col-12-mobile special">
									<a class="image fit">
										<img src="images/my/p4/human_exmp.jpg" /></a>
										
								</article>
								<article class="col-4 col-12-mobile special">
							<h3>
								ROLE

							</h3>
							<p>
								1) Tested pre-trained DNN model on our dataset 
							</p>
							<p>
								2) Data analysis among human behavior result, CNN model 1(trained on context dataset),and CNN model 2(trained on AffectNet)
							</p>
							<p>
								3) Paper writing
							</p>

							<h3>
								SKILLS UTILIZED

							</h3>
							<p style="margin-bottom: 5%;">
								Coding(python)
							</p>

						</article>
					</div>

					<section>

						<header>
							<h3>Purpose of this project</h3>
						</header>
						<p>
							Deep Neural Networks(DNNs) show similar performance levels to humans in the emotional recognition of face images. 
							Not only is the accuracy of DNNs' performance similar, but the focusing location is also akin to humans' performance. 
							Several studies have discovered this by comparing the locations where humans concentrate on inferring emotions 
							(using eye tracking) and where DNNs do (using GradCam).
							</p>
							<p>
								This study compared two types of DNN models and human behavior experiment results. 
								One of the two models was pre-trained using images including context 
								(i.e., facial expressions, body cues, background information), 
								while the other was pre-trained only on facial expression images. 
								Hence, the former model captures complete information in the provided static 
								images to infer emotions, while the latter infers emotions based on facial expressions.
							</p>
					</section>
					<section>
						<header>
							<h3>DNNs</h3>
						</header>
						<article class="col-10 col-12-mobile special">
						<table>
							<!-- <caption>Computational models</caption> -->
							<colgroup>
								<col style="background-color:darkseagreen;">
							</colgroup>
							<thead>
								<tr>
									<!-- <th rowspan="2">구분</th> -->
									<th colspan="7" style="background-color:wheat;">Computational models</th>
									<!-- <th></th> -->
									<!-- <th></th> -->
								</tr>
								<tr>
									<th></th>
									<th>Baseline</th>
									<th>Epochs</th>
									<th>Optimizer</th>
									<th>Learning Rate</th>
									<th>Batch size</th>
									<th>Loss function</th>
								</tr>
							</thead>
							<tfoot>
								<tr>
									<td colspan="4" style="background-color: white;"></td>
									<!-- <td></td> -->
									<!-- <td></td> -->
									<!-- <td></td> -->
								</tr>
							</tfoot>
							<tbody>
								<tr>
									<th>EMOTIC</th>
									<td>ResNet-18<br> AlexNet</td>
									<td>15</td>
									<td>Adam</td>
									<td>0.01</td>
									<td>52</td>
									<td>Smooth L1 (Dimensional emotion)<br> Weighted Euclidean loss (Discrete emotion)</td>
								</tr>
								<tr>
									<th>ResNet trained on re-labeled AffectNet dataset</th>
									<td>ResNet-50</td>
									<td>50</td>
									<td>Adam</td>
									<td>0.0001 </td>
									<td>64</td>
									<td>cross-entropy loss</td>
								</tr>
								
							</tbody>
						</table>
					</article>
						<i>Table. 1.</i><span style = " font-size:0.9em;">Specific description of computational models.</span>
						
					</section>
					<section>
						<header>
							<h3>Experiment</h3>
						</header>
						<p>
							<h1>Dataset</h1>
						</p>
						<p>
							The dataset included three types of images: full-context images 
							(images with complete information, face, body, and scene); 
							context blurred images (images with face and body); and face-only images 
							(images cropped to exclude all information except the face). 
							<p>
								<h1>Method</h1>
							</p>
							Since ResNet was trained on AffectNet (face images), we tested this model on a face-only image dataset. 
							Meanwhile, we tested full-context and context-blurred image types with the EMOTIC model.
							As such, we compared the results of two model types to those of human behavior experiments 
							(baseline) to examine how differently DNNs infer human emotions from how humans do. 
						</p>
						<p>
						</p>
						<p>
							
						</p>
					</section>

					<section>
						<header>
							<h3>Result</h3>
						</header>
						<h1>Discrete emotion</h1><br>

						<div class="row">

							<article class="col-10 col-12-mobile special">
								<a class="image fit"><img src="images/my/p4/cat.png" /></a>
								<i>Fig. 1.</i>
								<p>
									We discovered that the EMOTIC (context capture model) successfully classified images' emotions 
									in both full-context and context-blurred stimuli. 
									We learned from human behavior experiments that humans perceive full-context images more intensely. 
									For instance, when happiness emotion was given as two types of images (full-context and context-blurred), 
									humans perceived happiness emotion in full-context images more valence-positively than in context-blurred images. 
									Likewise, the EMOTIC model inferred full-context images as positive-valence-related emotions 
									(from affection to sympathy) and blurred-context images as negative-valence emotions (from anger to yearning).
								</p>

								<br><h1>Dimensional emotion</h1><br>
								<a class="image fit"><img src="images/my/p4/dim.png" /></a>
								<br><i>Fig. 2.</i> <span style = " font-size:0.9em;">
									Subtraction of each image ratings (from full-context images to face+body images).
									Left: human evaluation. Right: computational model (EMOTIC) rating. 
									Black circles are the differences between full-context and face+body images, 
									green circles are the differences between full-context and face-only images.
							</span>
								 <br>
								
								<p>
									
									Even though the model showed an outstanding result in categorical emotion inference, 
									it failed to produce a result akin to performance at the human level in dimensional emotions. 
									As demonstrated in Figure 2, the images that showed the most differences in the two dimensions 
									in human ratings were not the ones showing the most differences in computational model ratings. 
									Instead, these images were located in the center of the computational model's scatter plot, 
									meaning the most influential contexts for humans were considered the least influential 
									contexts for the computational model.
								</p>
							</article>
							
						</div>
						
				</article>


			</div>

		</div>


	</div>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.dropotron.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>